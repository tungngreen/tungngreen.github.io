
@misc{liebe2025fcpo,
	title = {{FCPO}: {Federated} {Continual} {Policy} {Optimization} for {Real}-{Time} {High}-{Throughput} {Edge} {Video} {Analytics}},
	shorttitle = {{FCPO}},
	url = {http://arxiv.org/abs/2507.18047},
	doi = {10.48550/arXiv.2507.18047},
	abstract = {The growing complexity of Edge Video Analytics (EVA) facilitates new kind of intelligent applications, but creates challenges in real-time inference serving systems. State-of-the-art (SOTA) scheduling systems optimize global workload distributions for heterogeneous devices but often suffer from extended scheduling cycles, leading to sub-optimal processing in rapidly changing Edge environments. Local Reinforcement Learning (RL) enables quick adjustments between cycles but faces scalability, knowledge integration, and adaptability issues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated RL (FRL) to address these challenges. This integration dynamically adjusts inference batch sizes, input resolutions, and multi-threading during pre- and post-processing. CRL allows agents to learn from changing Markov Decision Processes, capturing dynamic environmental variations, while FRL improves generalization and convergence speed by integrating experiences across inference models. FCPO combines these via an agent-specific aggregation scheme and a diversity-aware experience buffer. Experiments on a real-world EVA testbed showed over 5 times improvement in effective throughput, 60\% reduced latency, and 20\% faster convergence with up to 10 times less memory consumption compared to SOTA RL-based approaches.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Liebe, Lucas and Nguyen, Thanh-Tung and Lee, Dongman},
	month = jul,
	year = {2025},
	note = {arXiv:2507.18047 [cs] version: 1},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: 13 pages, 14 figures, 2 tables},
}
